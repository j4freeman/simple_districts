{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T22:08:12.107734Z",
     "start_time": "2019-12-04T22:08:08.865162Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PatchCollection\n",
    "import matplotlib\n",
    "\n",
    "from IPython.display import Image, Audio, clear_output\n",
    "\n",
    "import geopandas\n",
    "\n",
    "import pickle\n",
    "\n",
    "import zipfile\n",
    "\n",
    "import os\n",
    "\n",
    "import pysal as ps\n",
    "\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "import time\n",
    "\n",
    "import shutil\n",
    "\n",
    "from notify_run import Notify\n",
    "\n",
    "from shapely import geometry\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "from ast import literal_eval as make_tuple\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import decomposition\n",
    "from sklearn import cluster\n",
    "\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import imageio\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Data, DataLoader, Dataset\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "base_path = 'lawrence_leavenworth'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T22:08:12.214468Z",
     "start_time": "2019-12-04T22:08:12.109219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)\n",
    "\n",
    "GPU = True\n",
    "device_idx = 1\n",
    "if GPU:\n",
    "    device = torch.device(\"cuda:\" + str(device_idx - 1) if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T22:08:12.685018Z",
     "start_time": "2019-12-04T22:08:12.215215Z"
    }
   },
   "outputs": [],
   "source": [
    "douglas = geopandas.read_file(base_path)\n",
    "\n",
    "pkl = open(base_path + '_neighbors.pkl',\"rb\")\n",
    "neighbors = pickle.load(pkl)\n",
    "pkl.close()\n",
    "\n",
    "douglas['neighbors'] = neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-04T22:08:12.744846Z",
     "start_time": "2019-12-04T22:08:12.686957Z"
    }
   },
   "outputs": [],
   "source": [
    "douglas['populated'] = douglas.apply(lambda x: (x['2008_R'] + x['2008_D']) > 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.871Z"
    }
   },
   "outputs": [],
   "source": [
    "douglas.plot('His_Pct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.875Z"
    }
   },
   "outputs": [],
   "source": [
    "counties = []\n",
    "\n",
    "for i in tqdm_notebook(douglas.COUNTYFP.unique()):\n",
    "    county = douglas[douglas['COUNTYFP'] == i]['geometry']\n",
    "    poly = unary_union(county)\n",
    "    counties.append(poly)\n",
    "\n",
    "    \n",
    "county_frame = geopandas.GeoDataFrame([1 for _ in douglas.COUNTYFP.unique()], geometry=counties)\n",
    "\n",
    "base = douglas.plot('Urb_Pct', cmap='binary')\n",
    "base2 = county_frame.plot(0, edgecolor='black', cmap='gray', linewidth=0.35, facecolor='none', ax=base)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.877Z"
    }
   },
   "outputs": [],
   "source": [
    "edges = list(douglas.neighbors)\n",
    "adj_list = list(douglas.neighbors)\n",
    "for i in range(len(edges)):\n",
    "    edges[i] = [(i, e) for e in edges[i]]\n",
    "\n",
    "nodes = list(douglas.index)\n",
    "edges = [x for y in edges for x in y]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(nodes)\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "print(edges[:5])\n",
    "print(len(adj_list))\n",
    "print(G.number_of_nodes())\n",
    "nx.is_connected(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.879Z"
    }
   },
   "outputs": [],
   "source": [
    "counties = np.array(list(douglas['COUNTYFP'])).reshape(-1, 1)\n",
    "\n",
    "one_hots = preprocessing.LabelEncoder().fit_transform(counties)\n",
    "\n",
    "one_hots_tensor = torch.from_numpy(one_hots).long().to(device)\n",
    "\n",
    "one_hots_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.881Z"
    }
   },
   "outputs": [],
   "source": [
    "sources = []\n",
    "dests = []\n",
    "\n",
    "for i in G.edges(data=False):\n",
    "    sources.append(i[0])\n",
    "    dests.append(i[1])\n",
    "\n",
    "edges = [sources, dests]\n",
    "edges = torch.tensor(edges).to(device)\n",
    "\n",
    "print(edges.shape)\n",
    "\n",
    "attr_tensor = torch.load(base_path + '_data_tensor.pt').to(device)\n",
    "\n",
    "data = Data(x=attr_tensor, edge_index=edges, y=one_hots_tensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.882Z"
    }
   },
   "outputs": [],
   "source": [
    "# pop_2017 = list(pd.read_csv('2017_ks_pop/nhgis0010_ds233_20175_2017_blck_grp.csv')['AHY1E001'])\n",
    "\n",
    "# douglas['Total2'] = pop_2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.885Z"
    }
   },
   "outputs": [],
   "source": [
    "print(sum(douglas['Total']))\n",
    "# print(sum(douglas['Total2']))\n",
    "\n",
    "pop = list(douglas['Total'])\n",
    "\n",
    "data.x[:, -1] = torch.tensor(pop).to(device)\n",
    "attr_tensor[:, -1] = torch.tensor(pop).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.886Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, num_counties, hidden_size, input_dim, num_districts):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_counties = num_counties\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dim = input_dim\n",
    "        self.num_districts = num_districts\n",
    "        \n",
    "        self.conv1 = gnn.ChebConv(self.input_dim, self.hidden_size, 3)\n",
    "        self.lin1 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.conv2 = gnn.ChebConv(self.hidden_size, int(self.hidden_size/2), 2)\n",
    "#         self.conv3 = gnn.ChebConv(int(self.hidden_size/2), self.embedding_dim, 2)\n",
    "        self.conv3 = nn.Linear(int(self.hidden_size/2), self.embedding_dim)\n",
    "        \n",
    "        self.b1 = nn.BatchNorm1d(self.input_dim)\n",
    "        self.b2 = nn.BatchNorm1d(self.hidden_size)\n",
    "        self.b3 = nn.BatchNorm1d(int(self.hidden_size/2))\n",
    "        \n",
    "        self.b4 = nn.BatchNorm1d(int(self.hidden_size/2))\n",
    "        self.b5 = nn.BatchNorm1d(self.hidden_size)\n",
    "        self.b6 = nn.BatchNorm1d(self.input_dim)\n",
    "        \n",
    "        self.l1 = nn.BatchNorm1d(self.hidden_size)\n",
    "        self.l2 = nn.BatchNorm1d(self.hidden_size)\n",
    "        \n",
    "#         self.conv4 = gnn.ChebConv(self.embedding_dim, int(self.hidden_size/2), 2)\n",
    "        self.conv4 = nn.Linear(self.embedding_dim, int(self.hidden_size/2), 2)\n",
    "        self.conv5 = gnn.ChebConv(int(self.hidden_size/2), self.hidden_size, 2)\n",
    "        self.lin2 = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.conv6 = gnn.ChebConv(self.hidden_size, self.input_dim, 3)\n",
    "        \n",
    "        self.county_linear = nn.Linear(self.embedding_dim, self.num_counties)\n",
    "                \n",
    "        self.cluster_linear = nn.Linear(self.embedding_dim, self.num_districts)\n",
    "        \n",
    "        self.drop = nn.Dropout(0.125/self.embedding_dim)\n",
    "        \n",
    "        self.sp_linear = nn.Linear(self.embedding_dim, 1)\n",
    "        \n",
    "        self.activ = nn.ReLU()\n",
    "        \n",
    "    def encode(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "#         x = self.drop(x)\n",
    "#         x = x + torch.randn(x.shape).detach().to(device) * 0.1\n",
    "        \n",
    "#         noise = torch.rand(x.shape)\n",
    "#         x[noise < 0.025] = 0\n",
    "        \n",
    "        x = self.b1(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.activ(x)\n",
    "        x = self.b2(x)\n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.l1(x)\n",
    "        x = self.activ(x)\n",
    "        \n",
    "#         x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "                                  \n",
    "        x = self.activ(x)\n",
    "        x = self.b3(x)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def sp(self, x):\n",
    "        return self.sp_linear(x).squeeze()\n",
    "    \n",
    "    def decode(self, x, edge_index):\n",
    "        \n",
    "        x = self.drop(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.activ(x)\n",
    "                                  \n",
    "        x = self.conv5(x, edge_index)\n",
    "#         x = self.dropout(x)\n",
    "\n",
    "        x = self.activ(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        x = self.b5(x)\n",
    "        x = self.activ(x)\n",
    "        x = self.conv6(x, edge_index)\n",
    "        x = self.b6(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def predict_county(self, x, edge_index):\n",
    "\n",
    "        x = self.drop(x)\n",
    "        x = self.county_linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#     def cluster(self, y):\n",
    "#         y = y.double()\n",
    "#         q = torch.pow((1.0 + (torch.sum(torch.pow(torch.unsqueeze(y, dim=1) - self.centroids, 2), dim=2))), 2)\n",
    "#         q = q / torch.sum(q, dim=1, keepdim=True)\n",
    "        \n",
    "#         return q\n",
    "\n",
    "    def cluster(self, y, edge_index):\n",
    "#         y = self.activ(y)\n",
    "#         y = self.cluster_conv(y, edge_index)\n",
    "#         y = self.activ(y)\n",
    "        y = self.drop(y)\n",
    "        y = self.cluster_linear(y)\n",
    "        return y\n",
    "        \n",
    "\n",
    "    def forward(self, data):\n",
    "        _, edge_index = data.x, data.edge_index\n",
    "        encoded = self.encode(data)\n",
    "        decoded = self.decode(encoded, edge_index)\n",
    "        county_preds = self.predict_county(encoded, edge_index)\n",
    "        clusters = self.cluster(encoded, edge_index)\n",
    "\n",
    "        return encoded, decoded, county_preds, clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.888Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 6\n",
    "num_counties = douglas['COUNTYFP'].nunique()\n",
    "hidden_dim = 4096\n",
    "num_attrs = attr_tensor.shape[1]\n",
    "num_districts = 4\n",
    "\n",
    "\n",
    "weight_arr = np.ones(attr_tensor.shape[1])\n",
    "weight_arr[-1] = 0\n",
    "weight_arr[-2] = 0\n",
    "weight_arr[-3] = 0\n",
    "# weight_arr[5] = 0.5\n",
    "weights = torch.tensor(weight_arr).float().to(device)\n",
    "def weighted_mse(x, y):\n",
    "    se = nn.MSELoss(reduction='mean')(x*weights, y*weights)\n",
    "    return se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.889Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net(embedding_dim, num_counties,\n",
    "           hidden_dim, num_attrs, num_districts).to(device)\n",
    "\n",
    "# optim = torch.optim.Adamax(model.parameters(), lr=0.01)\n",
    "\n",
    "# encoded, decoded, county_preds, q = model(data)\n",
    "\n",
    "# centroids = np.random.choice(douglas.shape[0], num_districts)\n",
    "\n",
    "# arg_x = q.to(\"cpu\").detach()\n",
    "# douglas['district'] = np.argmax(arg_x, axis=1)\n",
    "# # print(len(douglas['district'].value_counts()))\n",
    "\n",
    "# comms = nx.community.asyn_fluidc(G, num_districts)\n",
    "\n",
    "# target_tensor = torch.zeros(douglas.shape[0]).to(device).long()\n",
    "\n",
    "# for idx, c in enumerate(comms):\n",
    "#     c = list(c)\n",
    "#     target_tensor[c] = idx\n",
    "    \n",
    "# while len(douglas['district'].value_counts()) != num_districts:\n",
    "#     optim.zero_grad()\n",
    "#     encoded, decoded, county_preds, q = model(data)\n",
    "    \n",
    "# #     loss = nn.CrossEntropyLoss()(q, target_tensor) + nn.MSELoss()(decoded, data.x)\n",
    "#     loss = nn.CrossEntropyLoss()(q, target_tensor) + nn.CrossEntropyLoss()(county_preds, data.y) + nn.MSELoss()(decoded, data.x) * 0.05\n",
    "    \n",
    "# #     print(loss.item(), len(douglas['district'].value_counts()))\n",
    "    \n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "    \n",
    "# for _ in tqdm_notebook(range(25)):\n",
    "#     optim.zero_grad()\n",
    "#     encoded, decoded, county_preds, q = model(data)\n",
    "    \n",
    "# #     loss = nn.CrossEntropyLoss()(q, target_tensor) + nn.MSELoss()(decoded, data.x)\n",
    "#     loss = nn.CrossEntropyLoss()(q, target_tensor) + nn.CrossEntropyLoss()(county_preds, data.y) + nn.MSELoss()(decoded, data.x) * 0.05\n",
    "    \n",
    "# #     print(loss.item(), len(douglas['district'].value_counts()))\n",
    "    \n",
    "#     loss.backward()\n",
    "#     optim.step()\n",
    "    \n",
    "# arg_x = q.to(\"cpu\").detach()\n",
    "# douglas['district'] = np.argmax(arg_x, axis=1)\n",
    "# douglas.plot('district')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.891Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "final_epochs = 50\n",
    "learning_rate = 0.005\n",
    "\n",
    "if os.path.exists('runs_grp/'):\n",
    "    os.rename('runs_grp/', 'del/')\n",
    "    shutil.rmtree('del/')\n",
    "\n",
    "os.mkdir('runs_grp/')\n",
    "\n",
    "weight_arr = np.ones(attr_tensor.shape[1])\n",
    "weight_arr[-1] = 0\n",
    "weight_arr[-2] = 0\n",
    "weight_arr[-3] = 0\n",
    "# weight_arr[5] = 0.5\n",
    "weights = torch.tensor(weight_arr).float().to(device)\n",
    "\n",
    "def weighted_mse(x, y):\n",
    "    se = nn.MSELoss(reduction='mean')(x*weights, y*weights)\n",
    "    return se\n",
    "\n",
    "data = data.to(device)\n",
    "embeddings, _, _, _ = model(data)\n",
    "embeddings = embeddings.detach()\n",
    "embeddings_np = embeddings.to(\"cpu\").to(\"cpu\")\n",
    "\n",
    "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# optim = torch.optim.Adamax(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def size_con(x, y):\n",
    "    x = torch.softmax(x, dim=1)\n",
    "#     x = torch.sigmoid(x)\n",
    "    \n",
    "#     x = x * y.unsqueeze(1).repeat(1, x.shape[1]).double()\n",
    "#     summed = torch.sum(x, dim=0)\n",
    "#     return nn.L1Loss()(summed, torch.sum(y) * torch.ones(summed.shape).double().to(device) / num_districts).float()\n",
    "    \n",
    "    x = x * y.unsqueeze(1).repeat(1, x.shape[1]).double()\n",
    "    summed = torch.mean(x, dim=0)\n",
    "    summed = summed / (torch.sum(summed) + 1e-10)\n",
    "    \n",
    "    return nn.L1Loss()(summed, torch.ones(summed.shape).double().to(device) / num_districts).float()\n",
    "\n",
    "def size_con2(x, coords):\n",
    "    arg_x = x.to(\"cpu\").detach()\n",
    "    douglas['district'] = np.argmax(arg_x, axis=1)\n",
    "#     print(douglas['district'].value_counts())\n",
    "    loss = 0\n",
    "    x = torch.softmax(x, dim=1)\n",
    "    target = torch.ones(1).to(device) / num_districts\n",
    "    tot = torch.sum(coords)\n",
    "    target = tot / tot / num_districts\n",
    "\n",
    "    for i in range(num_districts):\n",
    "        vals = douglas[douglas['district'] == i].index\n",
    "        subset = coords[vals]\n",
    "        subset = torch.sum(subset) / tot\n",
    "        \n",
    "        loss += nn.L1Loss(reduction='sum')(subset, target)\n",
    "        \n",
    "    if loss.item() != loss.item():\n",
    "        print(\"size failed\")\n",
    "        return\n",
    "    return loss\n",
    "\n",
    "def compact(x, coords):\n",
    "    arg_x = x.to(\"cpu\").detach()\n",
    "    douglas['district'] = np.argmax(arg_x, axis=1)\n",
    "#     print(len(douglas['district'].value_counts()))\n",
    "    loss = 0\n",
    "    x = torch.softmax(x, dim=1)\n",
    "#     x = torch.sigmoid(x)\n",
    "\n",
    "    for i in range(num_districts):\n",
    "#         all_polys = geometry.multipolygon.MultiPolygon(list(douglas[douglas['district'] == i]['geometry']))\n",
    "#         center = all_polys.centroid\n",
    "#         centroid_tensor = torch.tensor((center.x, center.y)).to(device)\n",
    "        \n",
    "        vals = douglas[douglas['district'] == i].index\n",
    "        centroid_tensor = coords[vals]\n",
    "        subset = coords[vals]\n",
    "        centroid_tensor = torch.mean(centroid_tensor, dim=0)\n",
    "        \n",
    "#         dists = nn.MSELoss()(subset, centroid_tensor * torch.ones(subset.shape).to(device)).detach()\n",
    "#         centroid = torch.argmin(dists)\n",
    "#         centroid_tensor = subset[centroid]\n",
    "        \n",
    "#         print(x.shape)\n",
    "#         print(coords.shape)\n",
    "#         print(centroid_tensor.shape)\n",
    "        \n",
    "        loss += nn.L1Loss()(coords * x[:, i].unsqueeze(1).repeat(1, 2), centroid_tensor * x[:, i].unsqueeze(1).repeat(1, 2))\n",
    "        \n",
    "    if loss.item() != loss.item():\n",
    "        print(\"compactness failed\")\n",
    "        return\n",
    "    return loss\n",
    "\n",
    "mask = list(douglas['Urb_Pct'])\n",
    "mask = torch.tensor(mask).float().to(device)\n",
    "\n",
    "def instance_con(x):\n",
    "    sums = torch.sum(torch.pow(x, 2), dim=1)\n",
    "    sums = mask * sums\n",
    "#     return torch.sum(sums).float() / mask.shape[0]\n",
    "    return torch.sum(sums).float() * 0.1\n",
    "\n",
    "def split(preds, counties):\n",
    "    preds = torch.softmax(preds, dim=1)\n",
    "    split_loss = 0\n",
    "    for k in range(douglas['COUNTYFP'].nunique()):\n",
    "        cur = preds[counties == k]\n",
    "        split_loss += torch.mean(torch.std(cur, dim=1))\n",
    "#         split_loss += nn.MSELoss(reduction='batchmean')(cur, cur[torch.randperm(cur.size()[0])])\n",
    "        \n",
    "    return split_loss\n",
    "\n",
    "losses = []\n",
    "kls = []\n",
    "recons = []\n",
    "sizes = []\n",
    "bces = []\n",
    "\n",
    "neg_list = [list(set(G.nodes) - set([n for n in G.neighbors(i)])) for i in G.nodes]\n",
    "        \n",
    "state = False\n",
    "    \n",
    "for i in tqdm_notebook(range(final_epochs)):\n",
    "        \n",
    "    total_loss = 0.\n",
    "    iters = 0.\n",
    "    size_loss = 0.\n",
    "    kl_loss = 0.\n",
    "    recon_loss = 0.\n",
    "    bce_loss = 0.\n",
    "\n",
    "    model.train()\n",
    "    optim.zero_grad()\n",
    "    \n",
    "\n",
    "    encoded, decoded, county_preds, q = model(data)\n",
    "    \n",
    "    if i%25 == 0:\n",
    "        douglas['district'] = np.argmax(q.to(\"cpu\").detach(), axis=1)\n",
    "        dists = []\n",
    "\n",
    "#         for j in range(num_districts):\n",
    "#             dist = douglas[douglas['district'] == j]['geometry']\n",
    "#             poly = unary_union(dist)\n",
    "#             dists.append(poly)\n",
    "#         dists = geopandas.GeoDataFrame(list(range(num_districts)), geometry=dists)\n",
    "#         base = douglas.plot('Urb_Pct', cmap='binary')\n",
    "#         base2 = county_frame.plot(0, edgecolor='black', cmap='gray', linewidth=0.35, facecolor='none', ax=base)\n",
    "#         dists.plot(0, edgecolor='black', cmap='tab20', linewidth=0.35, alpha=0.7, ax=base2)\n",
    "        \n",
    "        douglas.plot('district')\n",
    "        plt.title(\"argmax \" + str(i))\n",
    "        plt.savefig('runs_grp/' + str(i) + '.png')\n",
    "        plt.cla()\n",
    "        plt.clf()\n",
    "        plt.close('all')\n",
    "\n",
    "\n",
    "    rec = weighted_mse(decoded, data.x) * 0.5\n",
    "    \n",
    "    l1 = 0\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            l1 += torch.sum(torch.abs(p))\n",
    "            \n",
    "    l1 *= 0.1\n",
    "    l1 /= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#     kl = KL(p, q).float(\n",
    "    kl = size_con(q.double(), data.x[:,-1]) * 250\n",
    "#     kl = size_con2(q.double(), data.x[:,-1])\n",
    "    size = compact(q, attr_tensor[:,-3:-1]) * 5\n",
    "\n",
    "#     size = size_con(q, data.x[:,-1])\n",
    "#     size = torch.zero()\n",
    "\n",
    "    bce_l = nn.CrossEntropyLoss()(county_preds, data.y) * 10\n",
    "    \n",
    "#     bce_l = split(q, data.y)\n",
    "    \n",
    "#     compactness = compact()\n",
    "\n",
    "#     instance = instance_con(q)\n",
    "\n",
    "#         loss = rec + size + bce_l + edge + 0.1 * kl\n",
    "#     loss = bce_l + 2 * size + 0.1 * kl + rec + instance\n",
    "#     loss = bce_l + rec * 0.05 + size + kl * 0.1\n",
    "\n",
    "    neighbors = [int(np.random.choice([n for n in G.neighbors(i)])) for i in G.nodes]\n",
    "\n",
    "#     neighbor_loss = nn.CosineEmbeddingLoss()(q, q[neighbors], torch.ones(encoded.shape[0]).to(device) * 1)\n",
    "    neighbor_loss = nn.CosineEmbeddingLoss()(encoded, encoded[neighbors], torch.ones(encoded.shape[0]).to(device) * 1) * 5\n",
    "    for _ in range(5):\n",
    "#         negatives = [int(np.random.choice(range(G.number_of_nodes()))) for i in neg_list]\n",
    "        negatives = np.random.choice(G.number_of_nodes(), G.number_of_nodes())\n",
    "#         neighbor_loss += nn.CosineEmbeddingLoss()(q, q[negatives], torch.ones(encoded.shape[0]).to(device) * -1) / 6\n",
    "        neighbor_loss += nn.CosineEmbeddingLoss()(encoded, encoded[negatives], torch.ones(encoded.shape[0]).to(device) * -1)\n",
    "#     neighbor_loss *= 0.25\n",
    "    \n",
    "    neighbor_loss += nn.L1Loss()(q, q[neighbors])\n",
    "    \n",
    "    negatives = np.random.choice(G.number_of_nodes(), G.number_of_nodes())\n",
    "#     paths = [nx.shortest_path_length(G, i, negatives[i]) for i in G.nodes]\n",
    "#     paths = torch.tensor(paths).to(device).float()    \n",
    "    paths = attr_tensor[:,-3:-1] - attr_tensor[negatives][:,-3:-1]\n",
    "    paths = torch.pow(paths, 2)\n",
    "    paths = torch.sum(paths, dim=1)\n",
    "    paths = torch.sqrt(paths) * 5\n",
    "    \n",
    "    prods = model.sp((embeddings[negatives] + embeddings) / 2)\n",
    "\n",
    "    if i%5 == 0:\n",
    "        state = not state\n",
    "    \n",
    "    if state:\n",
    "        loss = bce_l + rec + size + neighbor_loss + nn.MSELoss()(prods, paths)\n",
    "    else:\n",
    "        loss = bce_l + rec + kl + neighbor_loss + nn.MSELoss()(prods, paths)\n",
    "    \n",
    "#     loss += size * ((i+1)**0.7 - 1) / 0.7\n",
    "#     loss += kl * ((i+1)**0.4 - 1) / 0.4\n",
    "#     loss += bce_l * ((i+1)**0.4 - 1) / 0.3\n",
    "    \n",
    "    if loss.item() != loss.item():\n",
    "        print(bce_l)\n",
    "        print(rec)\n",
    "        print(size)\n",
    "        print(kl)\n",
    "        break\n",
    "\n",
    "    if (i+1)%5 == 0:\n",
    "        print(\"Epoch: {:d} | Total Loss: {:.5f} | Compact: {:.5f} | Recon: {:.5f} | Pop: {:.5f} | BCE: {:.5f} | Adj: {:.5f}\".format(\n",
    "            i + 1, loss.item(), size.item(), rec.item(), kl.item(), bce_l.item(), nn.MSELoss()(prods, paths)))\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    sizes.append(size.item())\n",
    "    recons.append(rec.item())\n",
    "    kls.append(kl.item())\n",
    "    bces.append(bce_l.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "#     print(preds_idx)\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "\n",
    "plt.plot(range(len(sizes)), sizes)\n",
    "plt.title(\"compactness loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(recons)), recons)\n",
    "plt.title(\"recon loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(kls)), kls)\n",
    "plt.title(\"pop loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(bces)), bces)\n",
    "plt.title(\"bce loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.title(\"total loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.894Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "embeddings, _, _, clusters = model(data)\n",
    "\n",
    "print(clusters.shape)\n",
    "\n",
    "clusters_tensor = clusters\n",
    "clusters = clusters.to(\"cpu\").detach().numpy()\n",
    "\n",
    "soft_clusters = torch.softmax(clusters_tensor, dim=1).detach().to(\"cpu\").numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.896Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.softmax(clusters_tensor, dim=1).double()\n",
    "y = attr_tensor[:,-1].double()\n",
    "x = x * y.unsqueeze(1).repeat(1, x.shape[1]).double()\n",
    "summed = torch.mean(x, dim=0)\n",
    "summed = summed / (torch.sum(summed) + 1e-10)\n",
    "# summed\n",
    "print(soft_clusters[np.random.choice(clusters_tensor.shape[0], 5)])\n",
    "torch.abs(summed - torch.ones(summed.shape).double().to(device)/num_districts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.897Z"
    }
   },
   "outputs": [],
   "source": [
    "douglas['district'] = np.argmax(soft_clusters, axis=1)\n",
    "\n",
    "dists = []\n",
    "centroids = []\n",
    "centroids2 = []\n",
    "coords = attr_tensor[:,-3:-1]\n",
    "\n",
    "for i in range(num_districts):\n",
    "    dist = douglas[douglas['district'] == i]['geometry']\n",
    "    poly = unary_union(dist)\n",
    "    dists.append(poly)\n",
    "    \n",
    "#     all_polys = geometry.multipolygon.MultiPolygon(list(douglas[douglas['district'] == i]['geometry']))\n",
    "#     center = all_polys.centroid\n",
    "#     centroids.append([center.x, center.y])\n",
    "    \n",
    "#     vals = douglas[douglas['district'] == i].index\n",
    "#     centroid_tensor = coords[vals]\n",
    "#     subset = coords[vals]\n",
    "#     centroids2.append(torch.mean(centroid_tensor, dim=0).to(\"cpu\").detach().numpy())\n",
    "\n",
    "centroids = np.array(centroids)\n",
    "centroids2 = np.array(centroids2)\n",
    "    \n",
    "dists = geopandas.GeoDataFrame(list(range(num_districts)), geometry=dists)\n",
    "\n",
    "base = douglas.plot('Urb_Pct', cmap='binary')\n",
    "base2 = county_frame.plot(0, edgecolor='black', cmap='gray', linewidth=0.35, facecolor='none', ax=base)\n",
    "dists.plot(0, edgecolor='black', cmap='tab20', linewidth=0.35, legend=True, alpha=0.7, ax=base2)\n",
    "# base2.scatter(centroids[:, 0], centroids[:, 1], color='lime')\n",
    "# base2.scatter(centroids2[:, 0], centroids2[:, 1], color='darkslategray')\n",
    "plt.savefig('final_grp.png', dpi=2160)\n",
    "plt.show()\n",
    "\n",
    "for i in sorted(douglas['district'].unique()):\n",
    "    print(i, douglas[douglas['district'] == i]['Total_Pop'].sum() / (douglas['Total_Pop'].sum()/num_districts), douglas[douglas['district'] == i]['Total_Pop'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.900Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "\n",
    "for i in tqdm_notebook(range(num_districts)):\n",
    "    douglas['district'] = soft_clusters[:,i]\n",
    "#     douglas.plot('district', ax=ax[i//4, i%4])\n",
    "    douglas.plot('district', legend=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-12-04T22:08:08.901Z"
    }
   },
   "outputs": [],
   "source": [
    "# files = sorted([int(x[:-4]) for x in os.listdir('runs_grp/')])\n",
    "# files = [str(x) + '.png' for x in files]\n",
    "\n",
    "# with imageio.get_writer('runs_grp/' + str(int(time.time())) + '.gif', mode='I', duration=0.25) as writer:\n",
    "#     for f in files:\n",
    "#         filename = 'runs_grp/' + f\n",
    "#         image = imageio.imread(filename)\n",
    "#         writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
